{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9564208-8d41-4a7b-a12e-b13f6b3e3027",
   "metadata": {},
   "source": [
    "# Predictive Data Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f419ae18-5559-43bb-8a6d-b0d1bec4d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, split, when, sin, cos\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer, Pipeline\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, Tokenizer\n",
    "from pyspark.ml.regression import LinearRegression, DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706a9825-4d00-420b-88a9-b6490506674b",
   "metadata": {},
   "source": [
    "## Read Hive tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b50299-c3da-4793-b2f1-9b9c87866aeb",
   "metadata": {},
   "source": [
    "### Connect to Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ecd01-d86b-4de2-b0c0-1e6ca3a67bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add here your team number teamx\n",
    "team = \"team13\"\n",
    "\n",
    "# Location of Hive database in HDFS\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "        .appName(\"{} - spark ML\".format(team)) \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse) \\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4a7e4-7934-4eb6-9b43-46bafd99a20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7982d97e-f132-4af8-ba23-c12848742735",
   "metadata": {},
   "source": [
    "### List all databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c5c347-762d-4e98-89d5-7fd80e79efed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW DATABASES;\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff740cc-e16f-4338-913b-38c2c3181759",
   "metadata": {},
   "source": [
    "### List all tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66ed2a-3c11-43be-b564-8c48facf7a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.catalog.listTables(\"team13_projectdb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3edd2-c0b4-456f-82d1-2c033a8d5e41",
   "metadata": {},
   "source": [
    "### Read Hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93ce30-1b85-40c4-8d94-8c3ffcbc1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = spark.read.format(\"avro\").table('team13_projectdb.objects_part')\n",
    "\n",
    "fund_rounds = spark.read.format(\"avro\").table('team13_projectdb.funding_rounds_part')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a6c089-c75c-4974-a9a0-2466227583a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ML Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a04a0a-9d25-4655-9538-e7a2abd02fac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7659be-c6f9-47f1-a43f-9b9b75ee93f4",
   "metadata": {},
   "source": [
    "#### Feature selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100093a7-e039-412d-a123-217c8995e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and label\n",
    "obj_features = ['id', 'status', 'investment_rounds', 'invested_companies', 'milestones', 'relationships']\n",
    "\n",
    "fund_features = ['object_id', 'funded_at', 'funding_round_type', 'participants', 'is_first_round', 'is_last_round']\n",
    "label = 'raised_amount_usd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e09cf2-a117-4667-92a8-87df44f6b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = objects.select(obj_features)\n",
    "fund_rounds = fund_rounds.select(fund_features + [label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62b31f1-160e-4aa3-8a54-ee7ccac7fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join tables to form one Dataframe for the ML task\n",
    "final = objects.join(fund_rounds, objects['id'] == fund_rounds['object_id'], how='right').drop('id').drop('object_id')\n",
    "\n",
    "final.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173d8a11-c009-4a18-a63f-00ca03731772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split funded_at with datetime to year, month and day\n",
    "split_col = F.split(F.to_date(\"funded_at\"), \"-\")\n",
    "final = final.withColumn(\"funded_year\", split_col.getItem(0).cast(IntegerType())) \\\n",
    "            .withColumn(\"funded_month\", split_col.getItem(1).cast(IntegerType())) \\\n",
    "            .withColumn(\"funded_day\", split_col.getItem(2).cast(IntegerType()))\n",
    "final = final.drop(\"funded_at\")\n",
    "\n",
    "# Drop missing values\n",
    "final = final.na.drop()\n",
    "\n",
    "# Rename label\n",
    "final = final.withColumnRenamed(\"raised_amount_usd\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78b23a-4cfe-4db6-b026-0de99b4a63b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201afa64-ca41-43ee-a8fd-afd3d1225c5e",
   "metadata": {},
   "source": [
    "#### Building the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b040e880-03e7-44a3-ab80-02662b66509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract categorical, numerical and cyclical features\n",
    "categorical_cols = ['status', 'funding_round_type']\n",
    "\n",
    "numerical_cols = ['investment_rounds', 'invested_companies', 'milestones', 'relationships', 'funded_year', 'participants', 'is_first_round', 'is_last_round']\n",
    "\n",
    "cyclical_cols = ['funded_month', 'funded_day']\n",
    "periods = [12, 31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87304e13-0cf5-42b6-8cd5-692542aea18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a custom tranformer to encode cyclical features\n",
    "class CyclicTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    inputCol = Param(Params._dummy(), \"inputCol\", \"input column name.\", typeConverter=TypeConverters.toString)\n",
    "    outputCol = Param(Params._dummy(), \"outputCol\", \"output column name.\", typeConverter=TypeConverters.toString)\n",
    "  \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol: str = \"input\", outputCol: str = \"output\", period: int = 12):\n",
    "        super(CyclicTransformer, self).__init__()\n",
    "        self._setDefault(inputCol=None, outputCol=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        del(kwargs[\"period\"])\n",
    "        self.set_params(**kwargs)\n",
    "        self.period = period\n",
    "    \n",
    "    @keyword_only\n",
    "    def set_params(self, inputCol: str = \"input\", outputCol: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "    \n",
    "    def getInputCol(self):\n",
    "        return self.getOrDefault(self.inputCol)\n",
    "  \n",
    "    def getOutputCol(self):\n",
    "        return self.getOrDefault(self.outputCol)\n",
    "  \n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_col = self.getInputCol()\n",
    "        output_col = self.getOutputCol()\n",
    "        \n",
    "        sin_col = sin(2 * math.pi * df[input_col] / self.period) \n",
    "        cos_col = cos(2 * math.pi * df[input_col] / self.period)\n",
    "       \n",
    "        return df.withColumn(output_col + \"_sin\", sin_col).withColumn(output_col + \"_cos\", cos_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc866e-010f-423c-94af-c9a8c318da15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create String indexer to assign index for the string fields where each unique string will get a unique index\n",
    "indexers = [StringIndexer(inputCol=c, \n",
    "                           outputCol=\"{0}_indexed\".format(c)).setHandleInvalid(\"skip\") for c in categorical_cols]\n",
    "\n",
    "# Encode strings using One Hot encoding\n",
    "encoders = [OneHotEncoder(inputCol=indexer.getOutputCol(), \n",
    "                           outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers]\n",
    "\n",
    "# Encode cyclical features using custom Cyclic Transformer\n",
    "cyclic_transformers = [CyclicTransformer(inputCol=col, \n",
    "                                         outputCol=col + \"_cyc_enc\", \n",
    "                                         period=period) for col, period in zip(cyclical_cols, periods)]\n",
    "\n",
    "# This will concatenate the input cols into a single column\n",
    "assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + [transformer.getOutputCol() for transformer in cyclic_transformers] + numerical_cols, outputCol= \"features\")\n",
    "\n",
    "# Create a pipeline to use only a single fit and transform on the data\n",
    "pipeline = Pipeline(stages=indexers + encoders + cyclic_transformers + [assembler])\n",
    "\n",
    "# Fit the pipeline ==> This will call the fit functions for all transformers if exist\n",
    "model = pipeline.fit(final)\n",
    "\n",
    "# Fit the pipeline ==> This will call the transform functions for all transformers\n",
    "data = model.transform(final)\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c32620-1fe3-4f38-a961-9cbd8eafbf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all features and keep only the features and label columns\n",
    "data = data.select([\"features\", \"label\"])\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fd1cf-ec02-45ce-833e-c942fdc9c7a8",
   "metadata": {},
   "source": [
    "#### Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a51cca6-484d-468c-9fef-19b8d1265678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into 60% training and 40% test (it is not stratified)\n",
    "(train_data, test_data) = data.randomSplit([0.6, 0.4], seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39e910a-6fdb-4ab8-a08b-98f7ed48f31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to run commands\n",
    "def run(command):\n",
    "    return os.popen(command).read()\n",
    "\n",
    "train_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/train/*.json > data/train.json\")\n",
    "\n",
    "test_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/data/test/*.json > data/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2e08cf-dc38-47ef-ba12-95ac750977bb",
   "metadata": {},
   "source": [
    "###  Modeling: First model\n",
    "\n",
    "First model is Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77822e83-d529-4dbe-a6d5-d94cf0387325",
   "metadata": {},
   "source": [
    "#### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2fda76-fc2a-44b5-92ab-a510eb47184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Linear Regression Model\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit the data to the lr model\n",
    "model_lr = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eeb103-ace9-46d7-a978-a2f253c86157",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7588ce1-7118-4467-9b9a-1abd182898c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data (Prediction)\n",
    "predictions = model_lr.transform(test_data)\n",
    "\n",
    "# Display the predictions\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9c359-9a96-4274-8c31-ab5a5d877ac2",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71665ee1-70b8-4736-8610-fcf8a6d6e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model\n",
    "evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse_lr = evaluator1_rmse.evaluate(predictions)\n",
    "r2_lr = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse_lr))\n",
    "print(\"R2 on test data = {}\".format(r2_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d52124-da01-46b9-b522-9baa492c63f5",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b28b6e-1990-48b6-8efe-3531d16ad393",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(model_lr.aggregationDepth, [2, 3, 4]) \\\n",
    "                    .addGrid(model_lr.regParam, np.logspace(1e-3,1e-1)) \\\n",
    "                    .build()\n",
    "\n",
    "cv = CrossValidator(estimator=lr, \n",
    "                    estimatorParamMaps=grid, \n",
    "                    evaluator=evaluator1_rmse,\n",
    "                    parallelism=5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d202e-e8d6-46ee-beee-d939d797a310",
   "metadata": {},
   "source": [
    "#### Select the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6168cad-499f-4def-a4d6-59c6beda713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = bestModel\n",
    "pprint(model1.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d851f892-ff36-43fa-a175-a0e9d635d19e",
   "metadata": {},
   "source": [
    "#### Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd8bc3e-1b27-4ce2-b027-12c148555b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.write().overwrite().save(\"project/models/model1\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/models/model1 models/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8343a76-c232-41fe-9487-c2e719b56b45",
   "metadata": {},
   "source": [
    "#### Prediction of the best model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70cace-5fb2-4c4e-8d85-5493b8b2c8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model1.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee6b2a-4583-46b0-91bf-9858510a87a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model1_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model1_predictions.csv/*.csv > output/model1_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75172db9-2a86-4229-8136-8f854b2b58de",
   "metadata": {},
   "source": [
    "#### Evaluation of the best model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3769d14-45ef-4283-b76a-20625ca66fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the best model\n",
    "evaluator1_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator1_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse1 = evaluator1_rmse.evaluate(predictions)\n",
    "r2_1 = evaluator1_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse1))\n",
    "print(\"R2 on test data = {}\".format(r2_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f7ba0-e1d1-4426-b6ba-1f2a4d44e9ec",
   "metadata": {},
   "source": [
    "###  Modeling: Second model\n",
    "\n",
    "\n",
    "Second model is Decision Tree Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa93d7-63f5-46d8-a3aa-8b47e72304b9",
   "metadata": {},
   "source": [
    "#### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9a1412-24c5-4413-a3ae-89f89068fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision tree regression Model\n",
    "dt = DecisionTreeRegressor()\n",
    "\n",
    "# Fit the data to the lr model\n",
    "model_dt = dt.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ddb06c-4fec-4047-bcf7-a645436d68fa",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d21484-717b-4fcd-809c-e096f33c0263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data (Prediction)\n",
    "predictions = model_dt.transform(test_data)\n",
    "\n",
    "# Display the predictions\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99580d05-1477-414f-bd89-e35979fd19af",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c1247-da55-49b4-8fbe-79c59de0a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model\n",
    "evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse_dt = evaluator2_rmse.evaluate(predictions)\n",
    "r2_dt = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse_dt))\n",
    "print(\"R2 on test data = {}\".format(r2_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2019ae74-dc3d-473e-905f-2cf2e76c9786",
   "metadata": {},
   "source": [
    "#### Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb9bb8-5fbe-46c7-8c9d-27e12964b1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = ParamGridBuilder()\n",
    "grid = (ParamGridBuilder().addGrid(model_dt.maxDepth, [5, 10, 15, 20]) \\\n",
    "             .addGrid(model_dt.maxBins, [10, 20]) \\\n",
    "             .build())\n",
    "\n",
    "cv = CrossValidator(estimator=dt, \n",
    "                    estimatorParamMaps=grid, \n",
    "                    evaluator=evaluator2_rmse,\n",
    "                    parallelism=5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c7e2db-e91f-4769-8399-f9cf90871962",
   "metadata": {},
   "source": [
    "#### Select the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b819f1-4ce7-4fb2-bb4b-7af5cb8c615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = bestModel\n",
    "pprint(model2.extractParamMap())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4cad82-5920-4fae-bddb-7bc4439e6c8f",
   "metadata": {},
   "source": [
    "#### Save the model to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be798c64-57ee-4ed8-8b9d-239bb8d83a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.write().overwrite().save(\"project/models/model2\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -get project/models/model2 models/model2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa82c49-9926-4cea-8f5a-20fc887ce11b",
   "metadata": {},
   "source": [
    "#### Prediction of the best model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a80b4-ea9c-4f1e-922d-23a6713dff00",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model2.transform(test_data)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1012f2-ee21-4cfc-8670-d7e4718051c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model2_predictions.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/model2_predictions.csv/*.csv > output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a0a22-0105-4193-8408-c6b0ab76da7a",
   "metadata": {},
   "source": [
    "#### Evaluation of the best model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e02fb1a-8451-4081-ac0e-b7b9f8161cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the best model\n",
    "evaluator2_rmse = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "evaluator2_r2 = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "\n",
    "rmse2 = evaluator2_rmse.evaluate(predictions)\n",
    "r2_2 = evaluator2_r2.evaluate(predictions)\n",
    "\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = {}\".format(rmse2))\n",
    "print(\"R2 on test data = {}\".format(r2_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce31a86-8a4d-4a53-98f7-0045497ed80f",
   "metadata": {},
   "source": [
    "### Compare best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b65664-f4e7-4760-8ba9-fd1f26bdab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to report performance of the models\n",
    "models = [[str(model1), rmse1, r2_1], [str(model2), rmse2, r2_2]]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"model\", \"RMSE\", \"R2\"])\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abe1f6a-25de-43d6-b1b3-7b6d286f4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save it to HDFS\n",
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/evaluation.csv\")\n",
    "\n",
    "# Run it from root directory of the repository\n",
    "run(\"hdfs dfs -cat project/output/evaluation.csv/*.csv > output/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5335b-feea-4dcd-8bbc-b3440d6baf4d",
   "metadata": {},
   "source": [
    "## Stop spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1799175e-f184-4be2-bff7-066558669b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c8a9b-b26f-42f0-8614-bba28651ce2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
